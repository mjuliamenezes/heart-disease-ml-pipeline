{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908a428c",
   "metadata": {},
   "source": [
    "## 04 - Avalia√ß√£o e Compara√ß√£o dos Modelos\n",
    "\n",
    "Avaliar performance dos modelos no conjunto de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b277066",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fccb14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src import S3Client, ModelEvaluator, MLFlowClient, DatabaseClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4953070",
   "metadata": {},
   "source": [
    "# Carregar dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843b232",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "s3 = S3Client()\n",
    "\n",
    "X_test = s3.read_csv('processed/X_test_scaled.csv')\n",
    "y_test = s3.read_csv('processed/y_test.csv')['target']\n",
    "\n",
    "print(f\"üìä X_test: {X_test.shape}\")\n",
    "print(f\"üìä y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908dc96",
   "metadata": {},
   "source": [
    "# Inicializar evaluator e MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f80af7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator()\n",
    "mlflow_client = MLFlowClient(experiment_name=\"heart-disease-prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0970b",
   "metadata": {},
   "source": [
    "# Carregar Modelos Treinados do MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bccf816",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"üìã Modelos registrados no MLFlow:\")\n",
    "\n",
    "# Modelos do artigo\n",
    "article_models = ['knn', 'random_forest', 'logistic_regression', \n",
    "                  'svm', 'naive_bayes', 'decision_tree']\n",
    "\n",
    "# Modelos de melhoria\n",
    "improved_models = ['gradient_boosting', 'random_forest_tuned']\n",
    "\n",
    "all_model_names = article_models + improved_models\n",
    "\n",
    "for name in all_model_names:\n",
    "    print(f\"   - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546cd08",
   "metadata": {},
   "source": [
    "# Avaliar Modelos do Artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aa79ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in all_model_names:\n",
    "    try:\n",
    "        print(f\"\\nüîÑ Avaliando: {model_name}\")\n",
    "        \n",
    "        # Carregar modelo do MLFlow\n",
    "        model_uri = f\"models:/{model_name}/latest\"\n",
    "        model = mlflow_client.load_model(model_uri)\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"   ‚ö†Ô∏è Modelo n√£o encontrado: {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Avaliar\n",
    "        metrics = evaluator.evaluate_model(\n",
    "            y_test, \n",
    "            y_pred, \n",
    "            y_pred_proba,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        results.append(metrics)\n",
    "        \n",
    "        # Salvar m√©tricas no banco\n",
    "        db = DatabaseClient()\n",
    "        db.insert_model_metrics(\n",
    "            model_name=model_name,\n",
    "            model_version=\"1\",\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erro ao avaliar {model_name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(results)} modelos avaliados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633d39b",
   "metadata": {},
   "source": [
    "# Compara√ß√£o de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f26ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "comparison_df = evaluator.compare_models(results)\n",
    "\n",
    "print(\"\\nüìä Compara√ß√£o de Modelos:\")\n",
    "display(comparison_df[['model_name', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e7b91",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Ordenar por m√©trica\n",
    "    df_sorted = comparison_df.sort_values(metric, ascending=False)\n",
    "    \n",
    "    # Cores diferentes para modelos do artigo vs melhorias\n",
    "    colors = ['#3498db' if name in article_models else '#e74c3c' \n",
    "              for name in df_sorted['model_name']]\n",
    "    \n",
    "    ax.barh(df_sorted['model_name'], df_sorted[metric], color=colors)\n",
    "    ax.set_xlabel(metric.capitalize())\n",
    "    ax.set_title(f'Compara√ß√£o - {metric.upper()}')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Adicionar valores\n",
    "    for i, v in enumerate(df_sorted[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92167dd5",
   "metadata": {},
   "source": [
    "# Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5f5a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_metrics = evaluator.get_best_model_metrics(metric='accuracy')\n",
    "\n",
    "print(f\"\\nüèÜ Melhor Modelo: {best_metrics['model_name']}\")\n",
    "print(f\"   Accuracy:  {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {best_metrics['recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {best_metrics['f1_score']:.4f}\")\n",
    "if best_metrics.get('roc_auc'):\n",
    "    print(f\"   ROC AUC:   {best_metrics['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda76d0",
   "metadata": {},
   "source": [
    "## Matrizes de Confus√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c71ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "top_4_models = comparison_df.head(4)['model_name'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(top_4_models):\n",
    "    try:\n",
    "        # Carregar modelo\n",
    "        model_uri = f\"models:/{model_name}/latest\"\n",
    "        model = mlflow_client.load_model(model_uri)\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Matriz de confus√£o\n",
    "        cm = evaluator.get_confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Plotar\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                   xticklabels=['Saud√°vel', 'Doen√ßa'],\n",
    "                   yticklabels=['Saud√°vel', 'Doen√ßa'])\n",
    "        ax.set_title(f'Matriz de Confus√£o - {model_name}')\n",
    "        ax.set_ylabel('Real')\n",
    "        ax.set_xlabel('Predito')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao plotar CM para {model_name}: {str(e)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728a3ac",
   "metadata": {},
   "source": [
    "## Curvas ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc865a55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for model_name in top_4_models:\n",
    "    try:\n",
    "        # Carregar modelo\n",
    "        model_uri = f\"models:/{model_name}/latest\"\n",
    "        model = mlflow_client.load_model(model_uri)\n",
    "        \n",
    "        # Probabilidades\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "            \n",
    "            # Calcular ROC\n",
    "            roc_data = evaluator.calculate_roc_curve(y_test, y_pred_proba)\n",
    "            \n",
    "            if roc_data:\n",
    "                plt.plot(roc_data['fpr'], roc_data['tpr'], \n",
    "                        label=f\"{model_name} (AUC={roc_data['auc']:.3f})\",\n",
    "                        linewidth=2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao plotar ROC para {model_name}: {str(e)}\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.500)', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curvas ROC - Compara√ß√£o dos Melhores Modelos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6f1aa",
   "metadata": {},
   "source": [
    "# Classification Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0407b",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä CLASSIFICATION REPORTS - TOP 3 MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name in comparison_df.head(3)['model_name']:\n",
    "    try:\n",
    "        # Carregar modelo\n",
    "        model_uri = f\"models:/{model_name}/latest\"\n",
    "        model = mlflow_client.load_model(model_uri)\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Report\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MODELO: {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        report = evaluator.get_classification_report(\n",
    "            y_test, \n",
    "            y_pred,\n",
    "            target_names=['Saud√°vel (0)', 'Doen√ßa (1)']\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f02a7ce",
   "metadata": {},
   "source": [
    "### Feature Importance (Top 3 Tree-based Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df74bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tree_models = ['random_forest', 'decision_tree', 'gradient_boosting', 'random_forest_tuned']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "feature_names = X_test.columns.tolist()\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    try:\n",
    "        # Carregar modelo\n",
    "        model_uri = f\"models:/{model_name}/latest\"\n",
    "        model = mlflow_client.load_model(model_uri)\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            # Feature importance\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            # Criar DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False).head(15)\n",
    "            \n",
    "            # Plotar\n",
    "            ax = axes[idx]\n",
    "            ax.barh(importance_df['feature'], importance_df['importance'], color='steelblue')\n",
    "            ax.set_xlabel('Import√¢ncia')\n",
    "            ax.set_title(f'Feature Importance - {model_name}')\n",
    "            ax.invert_yaxis()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao plotar FI para {model_name}: {str(e)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938c01a",
   "metadata": {},
   "source": [
    "# Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13a042",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "evaluator.export_metrics('/home/jovyan/work/models/model_metrics.csv')\n",
    "\n",
    "# Upload para MinIO\n",
    "s3.upload_file('/home/jovyan/work/models/model_metrics.csv', 'models/model_metrics.csv')\n",
    "\n",
    "print(\"‚úÖ M√©tricas exportadas para MinIO!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bb05e",
   "metadata": {},
   "source": [
    "# Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90f8be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Resumo estat√≠stico\n",
    "summary = evaluator.get_metrics_summary()\n",
    "\n",
    "print(\"\\nüìä Resumo Estat√≠stico das M√©tricas:\")\n",
    "display(summary[['accuracy', 'precision', 'recall', 'f1_score']])\n",
    "\n",
    "# Compara√ß√£o Artigo vs Melhorias\n",
    "print(\"\\nüìä Compara√ß√£o: Modelos do Artigo vs Melhorias\")\n",
    "\n",
    "article_results = comparison_df[comparison_df['model_name'].isin(article_models)]\n",
    "improved_results = comparison_df[comparison_df['model_name'].isin(improved_models)]\n",
    "\n",
    "print(\"\\nüéØ MODELOS DO ARTIGO:\")\n",
    "print(f\"   Accuracy m√©dia: {article_results['accuracy'].mean():.4f}\")\n",
    "print(f\"   Melhor: {article_results.iloc[0]['model_name']} ({article_results.iloc[0]['accuracy']:.4f})\")\n",
    "\n",
    "print(\"\\nüöÄ MODELOS DE MELHORIA:\")\n",
    "print(f\"   Accuracy m√©dia: {improved_results['accuracy'].mean():.4f}\")\n",
    "print(f\"   Melhor: {improved_results.iloc[0]['model_name']} ({improved_results.iloc[0]['accuracy']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
